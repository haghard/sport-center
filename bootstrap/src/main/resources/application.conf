akka {

  actor {

    serializers {
      crawler-response = "crawler.serializer.CrawlerResponseSerializer"
      change-set-event = "domain.serializer.ChangeSetEventSerializer"
      result-added-event = "domain.serializer.ResultAddedEventSerializer"
      campaign-persisted-event = "domain.serializer.CampaignPersistedSerializer"
    }

    serialization-bindings {
      "crawler.writer.CrawlerGuardian$CrawlerResponse" = crawler-response
      "domain.update.DistributedDomainWriter$BeginTransaction" = change-set-event
      "domain.TeamAggregate$ResultAdded" = result-added-event
      "domain.CrawlerCampaign$CampaignPersistedEvent" = campaign-persisted-event
    }
  }

  logConfigOnStart = on

  logger-startup-timeout = 10000

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = DEBUG
  stdout-loglevel = DEBUG

  log-dead-letters = off

  actor {
    provider = akka.cluster.ClusterActorRefProvider
  }

  scheduler-dispatcher {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher
    # What kind of ExecutionService to use
    executor = "fork-join-executor"
    # Configuration for the fork join pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 2
      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0
      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 10
    }
    throughput = 100
  }

  http-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 4
      parallelism-factor = 2.0
      parallelism-max = 20
    }
    throughput = 100
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }

  cluster {
    seed-nodes = [
      "akka.tcp://SCenter@127.0.0.1:2551",
      "akka.tcp://SCenter@127.0.0.1:2552"]

    auto-down-unreachable-after = 10s
  }

  cluster.sharding {
    role = "Domain"
    #Timeout of the shard rebalancing process.
    remember-entities = on
    handoff-timeout = 60 s

    # How often the coordinator saves persistent snapshots, which are
    # used to reduce recovery times
    snapshot-interval = 60 s

    # Rebalance check is performed periodically with this interval
    rebalance-interval = 20 s
  }

  persistence {
    journal.plugin = "cassandra-journal"
    snapshot-store.plugin = "cassandra-snapshot-store"
  }
}

cassandra-journal {
  # FQCN of the cassandra journal plugin
  class = "akka.persistence.cassandra.journal.CassandraJournal"

  # Name of the keyspace to be created/used by the journal
  keyspace = "sport_center"

  # Name of the table to be created/used by the journal
  table = "sport_center_journal"

  # Replication factor to use when creating a keyspace
  replication-factor = 3

  # Write consistency level
  write-consistency = "QUORUM"

  # Read consistency level
  read-consistency = "QUORUM"

  # Maximum number of entries per partition (= columns per row).
  max-partition-size = 5000000

  # Maximum size of result set
  max-result-size = 50001

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "akka.actor.default-dispatcher"

  # Dispatcher for fetching and replaying messages
  replay-dispatcher = "akka.persistence.dispatchers.default-replay-dispatcher"
}

cassandra-snapshot-store {

  # FQCN of the cassandra snapshot store plugin
  class = "akka.persistence.cassandra.snapshot.CassandraSnapshotStore"

  # Name of the keyspace to be created/used by the snapshot store
  keyspace = "sport_center"

  # Name of the table to be created/used by the snapshot store
  table = "sport_center_snapshot"

  # Replication factor to use when creating a keyspace
  replication-factor = 3

  # Write consistency level
  write-consistency = "ONE"

  # Read consistency level
  read-consistency = "ONE"

  # Maximum number of snapshot metadata to load per recursion (when trying to
  # find a snapshot that matches specified selection criteria). Only increase
  # this value when selection criteria frequently select snapshots that are
  # much older than the most recent snapshot i.e. if there are much more than
  # 10 snapshots between the most recent one and selected one. This setting is
  # only for increasing load efficiency of snapshots.
  max-metadata-result-size = 10

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "cassandra-snapshot-store.default-dispatcher"

  # Default dispatcher for plugin actor.
  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
    }
  }
}

scheduler-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-factor = 2.0
    parallelism-max = 4
  }
  throughput = 100
}

stream-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-factor = 2.0
    parallelism-max = 4
  }
  throughput = 100
}

hystrix-stream-dispatcher {
  type = Dispatcher
  executor = fork-join-executor
  fork-join-executor {
    parallelism-min = 2
    parallelism-factor = 2.0
    parallelism-max = 4
  }
  throughput = 100
}

crawler-dispatcher {
  type = PinnedDispatcher
  executor = thread-pool-executor
}

discovery {
  http-port = [8000, 8100]
  ops-timeout = 3 s
}