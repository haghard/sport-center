akka {

  logConfigOnStart = on

  logger-startup-timeout = 10000
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  loglevel = "INFO" //modify loglevel in lagback.xml
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  log-dead-letters = off

  actor {
    provider = akka.cluster.ClusterActorRefProvider
  }

  scheduler-dispatcher {
    # Dispatcher is the name of the event-based dispatcher
    type = Dispatcher
    # What kind of ExecutionService to use
    executor = "fork-join-executor"
    # Configuration for the fork join pool
    fork-join-executor {
      # Min number of threads to cap factor-based parallelism number to
      parallelism-min = 2
      # Parallelism (threads) ... ceil(available processors * factor)
      parallelism-factor = 2.0
      # Max number of threads to cap factor-based parallelism number to
      parallelism-max = 10
    }
    throughput = 100
  }

  http-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 4
      parallelism-factor = 2.0
      parallelism-max = 20
    }
    throughput = 100
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 0
    }
  }

  cluster {
    seed-nodes = [
      "akka.tcp://SCenter@127.0.0.1:2551",
      "akka.tcp://SCenter@127.0.0.1:2552"]

    auto-down-unreachable-after = 10s
  }

  extensions = ["akka.contrib.datareplication.DataReplication"]

  persistence {
    journal.plugin = "casbah-journal"
    snapshot-store.plugin = "casbah-snapshot-store"
    view.auto-update-interval = 5s
  }

  contrib.cluster.sharding {
    role = "microservice"
    #Timeout of the shard rebalancing process.
    handoff-timeout = 60 s

    # How often the coordinator saves persistent snapshots, which are
    # used to reduce recovery times
    snapshot-interval = 60 s

    # Rebalance check is performed periodically with this interval
    rebalance-interval = 20 s
  }

  stream.materializer {
    initial-input-buffer-size = 4
    max-input-buffer-size = 16
    initial-fan-out-buffer-size = 4
    max-fan-out-buffer-size = 16
    #dispatcher = http-dispatcher
    #file-io-dispatcher = http-dispatcher
  }
}

singletons {

  crawler {
    proxyName = crawlerProxy
    singletonName = crawler
    name = crawler-singleton
  }

  updater {
    proxyName = updaterProxy
    singletonName = updater
    name = updater-singleton
  }

  twitter {
    proxyName = tweeterProxy
    singletonName = tweeter
    name = tweeter-singleton
  }

}

broker-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 4
    parallelism-factor = 2.0
    parallelism-max = 20
  }
  throughput = 100
}

discovery {
  http-port = [8000, 8100]
  ops-timeout = 2 s
}

scheduler-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-factor = 2.0
    parallelism-max = 4
  }
  throughput = 100
}

stream-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-factor = 2.0
    parallelism-max = 4
  }
  throughput = 100
}


io-dispatcher {
  type = PinnedDispatcher
  executor = thread-pool-executor
}

spray {
  can.server {
    idle-timeout = 90 s
    request-timeout = 80 s
    connection-timeout = 90 s
    request-chunk-aggregation-limit = 0
    response-size-hint = 10
  }

  can.client {
    idle-timeout = 90 s
    request-timeout = 80 s
    connection-timeout = 90 s
    response-chunk-aggregation-limit = 0
  }
}

casbah-journal.mongo-journal-url = "mongodb://192.168.0.180:27017/nba.journal"
casbah-snapshot-store.mongo-snapshot-url = "mongodb://192.168.0.180:27017/nba.snapshots"
casbah-journal.mongo-journal-write-concern = "acknowledged"
casbah-journal.mongo-journal-write-concern-timeout = 3000

broker {
  sender-port = 8111
  receiver-port = 8112
}